{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "common-cu110.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Sample_NoteBook_15122020 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-leVRcbzoskg"
      },
      "source": [
        "# This is a short notebook to show sample code for my master's application to MILA. For this sample, I decided to show case the training of a DenseNet model. \n",
        "\n",
        "## Thank you for taking the time to read this. Feel free to reach out if you have any questions or comments regarding this. \n",
        "\n",
        "#### Vincent Dandenault - vincent.dandenault@polymtl.ca "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMclDCYLoskm"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89jg3Jeloskm",
        "outputId": "b4aa671c-7d93-464f-98f0-13243a3e9ae0"
      },
      "source": [
        "%capture \n",
        "!pip install IPython "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%capture` not found (But cell magic `%%capture` exists, did you mean that instead?).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "fvBBlwYzoskn",
        "outputId": "200a4d55-a124-4aca-d4b0-2e27f11fdf4f"
      },
      "source": [
        "%autosave 30"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(30000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autosaving every 30 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1FmWUeDosko",
        "outputId": "6b792e29-dbd6-46c6-f402-928d017cdf37"
      },
      "source": [
        "!pip3 install keras_tqdm\n",
        "!pip3 install utils2\n",
        "!pip3 install keras\n",
        "!pip3 install tensorflow\n",
        "!pip3 install setuptools\n",
        "!pip3 install keras \n",
        "!pip3 install -U scikit-learn\n",
        "!pip3 install numpy\n",
        "!pip3 install scipy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (2.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras_tqdm) (1.15.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n",
            "Collecting utils2\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/45/a560ae745719bb6aa16c416643a5b20a73b379ec93e9b1718d11f07126aa/utils2-0.1.2.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from utils2) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from utils2) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from utils2) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->utils2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->utils2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->utils2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->utils2) (2020.12.5)\n",
            "Building wheels for collected packages: utils2\n",
            "  Building wheel for utils2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils2: filename=utils2-0.1.2-py2.py3-none-any.whl size=5241 sha256=4b1de9fd1c59b757cac928b91453df65a5729560448a5b205c195e0164e8ba0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/1c/15/f037ee255bad89d0652e4238714086f405990a6687d5148f99\n",
            "Successfully built utils2\n",
            "Installing collected packages: utils2\n",
            "Successfully installed utils2-0.1.2\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.34.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (50.3.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1txPLXvosko",
        "outputId": "1905b248-40fe-4c2f-f4f9-fbfaffb7ec69"
      },
      "source": [
        "!python -m pip show scikit-learn "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: scikit-learn\n",
            "Version: 0.23.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: threadpoolctl, scipy, numpy, joblib\n",
            "Required-by: yellowbrick, umap-learn, textgenrnn, sklearn, sklearn-pandas, mlxtend, lucid, lightgbm, librosa, imbalanced-learn, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veagTS10oskp"
      },
      "source": [
        "#Tools\n",
        "import importlib\n",
        "import utils2\n",
        "from utils2 import *\n",
        "import os \n",
        "import cv2\n",
        "import json\n",
        "from multiprocessing import Process\n",
        "from multiprocessing import Queue\n",
        "\n",
        "# Libs\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#tf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "#Keras \n",
        "import keras as keras\n",
        "from keras import regularizers\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.models import load_model\n",
        "from keras.layers.merge import add, concatenate, Multiply\n",
        "from keras import regularizers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "#sklearn\n",
        "import sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#SciPy\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from scipy.ndimage import map_coordinates"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNklQ4Owoskp"
      },
      "source": [
        "## Import Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_NYlhXxoskp"
      },
      "source": [
        "PATH = ''"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErDOc_Meoskq"
      },
      "source": [
        "def load_data():\n",
        "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "    return (train_images, train_labels), (test_images, test_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEsBnqjxoskq",
        "outputId": "117b942a-2620-4c52-aa63-7ea4bd99f935"
      },
      "source": [
        " (x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtuDCNl8oskq",
        "outputId": "18632199-e292-4eec-d29d-6586aac69bcf"
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wSm6Wlooskq"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, name, preprocess = False, amplitude = [5], stdDev = [2], rotate_augment = False, angles = [60]):\n",
        "        self.name = name\n",
        "        self.isPreprocessed = preprocess\n",
        "        self.amplitude = amplitude\n",
        "        self.stdDev = stdDev\n",
        "        self.angles = angles\n",
        "        self.IMAGE_SIZE_CROP = 32\n",
        "        self.IMAGE_SIZE = 100\n",
        "        \n",
        "        (x_train, y_train), (x_test, y_test) = load_data()\n",
        "        \n",
        "        x_train = x_train/255.\n",
        "        x_test = x_test/255.\n",
        "        \n",
        "        self.X_train = x_train\n",
        "        \n",
        "        self.oneHotTransformer = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
        "        \n",
        "        self.Y_train = self.oneHotTransformer.fit_transform(y_train.reshape(-1, 1))\n",
        "        self.y_test = self.oneHotTransformer.fit_transform(y_test.reshape(-1, 1))\n",
        "    \n",
        "        self.splitData()\n",
        "        \n",
        "        if rotate_augment == True:\n",
        "            print(\"Augmentation of the dataset by rotation\")\n",
        "            Y_list = []\n",
        "            q = Queue()\n",
        "            p_ = []\n",
        "            for angle in self.angles:\n",
        "                p = Process(target=self.applyRotationToDataset, args=(X_train_original, angle, q,))\n",
        "                p_.append(p)\n",
        "                p.start()\n",
        "                print('*', end='')\n",
        "                Y = np.asarray(list(Y_train_original))\n",
        "                Y_list.append(Y)\n",
        "                print('\\n preparing for join')\n",
        "      \n",
        "            for i in range(0,len(Y_list)):\n",
        "                self.X_train = np.asarray(list(self.X_train) + list(q.get()))\n",
        "                self.Y_train = np.asarray( list(self.Y_train) + list(Y_list[i]))\n",
        "\n",
        "            for p in p_:\n",
        "                p.join()\n",
        "                print('*', end='')\n",
        "        \n",
        "    def convertData(self, data):\n",
        "        images = []\n",
        "        for dataLine in data:\n",
        "            if self.isPreprocessed == False:\n",
        "                images.append(np.reshape(dataLine[1], (self.IMAGE_SIZE, self.IMAGE_SIZE, 1)))\n",
        "            else:\n",
        "                images.append(np.reshape(dataLine, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1)))\n",
        "        \n",
        "        return np.asarray(images)\n",
        "\n",
        "    def visualizeData(self, indexBegin, indexEnd, dataset = 'X_train'):\n",
        "        if dataset == 'X_train':\n",
        "            data = self.X_train\n",
        "        elif dataset == 'X_valid':\n",
        "            data = self.X_valid\n",
        "        elif dataset == 'X_test':\n",
        "            data = self.X_test\n",
        "        elif dataset == 'X_submission':\n",
        "            data = self.X_submission\n",
        "    \n",
        "        for index in range(indexBegin, indexEnd):\n",
        "            plt.imshow(np.reshape(data[index], (self.IMAGE_SIZE_CROP,self.IMAGE_SIZE_CROP)), cmap='Greys')\n",
        "            plt.show()\n",
        "\n",
        "    def splitData(self):\n",
        "        self.X_train, self.X_valid, self.Y_train, self.Y_valid = train_test_split(self.X_train, self.Y_train, test_size=0.20, random_state=42)\n",
        "  \n",
        "    def applyRotationToDataset(self, dataset, angle, q):\n",
        "        rotated_dataset = []\n",
        "        for img in dataset:\n",
        "            img = np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP))\n",
        "            img = self.rotate_image(img, angle)\n",
        "            rotated_dataset.append(np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1)))\n",
        "        q.put(np.asarray(rotated_dataset))\n",
        "\n",
        "    def rotate_image(self, image, angle):\n",
        "        center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        result = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "        return result\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7-XqkWNoskr"
      },
      "source": [
        "dataset = Dataset('Data', False, amplitude = None, stdDev = None, rotate_augment = False, angles = None)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNGb_I-4oskr",
        "outputId": "9985ff44-da3d-4729-9867-318d602efe6c"
      },
      "source": [
        "print(dataset.Y_train[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBZtoqZyosks"
      },
      "source": [
        "## Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8AqPNttosks"
      },
      "source": [
        "class TrainableModel:\n",
        "    def __init__(self, dataset, model, name = \"\", patience = 15):\n",
        "        self.name = name\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "        self.es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
        "        self.mc = ModelCheckpoint(PATH + 'best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "        self.optimizer = 'adam'\n",
        "        self.compileModel()\n",
        "\n",
        "    def evaluate(self):\n",
        "        #self.loadBestModel()\n",
        "        results = self.model.evaluate(self.dataset.X_test, self.dataset.Y_test, batch_size=128)\n",
        "        self.test_accuracy = results[1]\n",
        "        print(results)\n",
        "        with open(PATH + 'History_' + str(self.name) + '_' + str(self.dataset.name) + '_' + str(self.test_accuracy) + '.json', 'w') as f:\n",
        "            json.dump(self.history.history, f)\n",
        "    \n",
        "    def compileModel(self):\n",
        "        self.model.compile(optimizer= self.optimizer, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "    \n",
        "    def softmaxVectorToOneHot(self, vector):\n",
        "        index = np.argmax(vector, axis=0)\n",
        "        oneHot = np.zeros(31)\n",
        "        oneHot[index] = 1\n",
        "        return oneHot\n",
        "\n",
        "    def submission(self):\n",
        "        #self.loadBestModel()\n",
        "        Y_predicted = self.model.predict(self.dataset.X_submission)\n",
        "        labels = []\n",
        "        for line in Y_predicted:\n",
        "            labels.append(self.dataset.oneHotTransformer.inverse_transform(self.softmaxVectorToOneHot(line).reshape(1, -1))[0][0])\n",
        "        df = pd.DataFrame()\n",
        "        df['Category'] = labels\n",
        "        df.insert(0, 'Id', range(0, len(df)))\n",
        "        df.to_csv(PATH + 'Submission-' + str(self.test_accuracy) + '.csv',index=False)\n",
        "\n",
        "    def fit(self, num_epochs = 300, batchSize = 16):\n",
        "        X = self.dataset.X_train\n",
        "        Y = self.dataset.Y_train\n",
        "        X_valid = self.dataset.X_valid\n",
        "        Y_valid = self.dataset.Y_valid\n",
        "        self.history = self.model.fit(X, Y, epochs=num_epochs, batch_size = batchSize, validation_data=(X_valid, Y_valid), callbacks=[self.es, self.mc])\n",
        "\n",
        "    def loadBestModel(self):\n",
        "        self.model = load_model(PATH + 'best_model.h5')\n",
        "\n",
        "    def fitOverAllData(self, num_epochs = 5):\n",
        "        X = self.dataset.X_ALL\n",
        "        Y = self.dataset.Y_ALL\n",
        "        self.history = self.model.fit(X, Y, epochs=num_epochs)\n",
        "\n",
        "    def graphFittingLoss(self):\n",
        "        plt.plot(self.history.history['loss'])\n",
        "        plt.plot(self.history.history['val_loss'])\n",
        "        plt.title('Model loss over the training steps')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    def graphFittingAccuracy(self):\n",
        "        plt.plot(self.history.history['accuracy'])\n",
        "        plt.plot(self.history.history['val_accuracy'])\n",
        "        plt.title('Model accuracy over the training steps')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FdjMYVLoskt"
      },
      "source": [
        "## Dense Net "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLkRdoTFoskt"
      },
      "source": [
        "RUN = True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPolnll5oskt"
      },
      "source": [
        "#This implementation of the DenseNet architecture is based on the fastai course\n",
        "#https://github.com/fastai/courses/blob/master/deeplearning2/densenet-keras.ipynb\n",
        "\n",
        "def convolution(x, numberFilters, filterSize, weigthDecay, p):\n",
        "    x = layers.Conv2D(numberFilters, (filterSize, filterSize), kernel_initializer='uniform', padding='same', kernel_regularizer = tf.keras.regularizers.l2(weigthDecay))(x)\n",
        "    return layers.Dropout(p)(x) if p else x\n",
        "\n",
        "def convolutionBlock(x, numberFilters, bottleneck=False, p=None, weigthDecay=0):\n",
        "    x = layers.Activation('relu')(layers.BatchNormalization(axis=-1)(x))\n",
        "    if bottleneck: x = layers.Activation('relu')(layers.BatchNormalization(axis=-1)(convolution(x, numberFilters * 4, 1, weigthDecay, p)))\n",
        "    return convolution(x, numberFilters, 3, weigthDecay, p)\n",
        "\n",
        "def denselyConnectedBlock(x, numberOfLayers, growthRate, bottleneck=False, p=None, weigthDecay=0):\n",
        "    if bottleneck: numberOfLayers //= 2\n",
        "    for i in range(numberOfLayers):\n",
        "        b = convolutionBlock(x, growthRate, bottleneck=bottleneck, p=p, weigthDecay=weigthDecay)\n",
        "        x = concatenate([x,b], -1)\n",
        "    return x\n",
        "\n",
        "def TransBlock(x, compression=1.0, p=None, weigthDecay=0):\n",
        "    numberFilters = int(x.get_shape().as_list()[-1] * compression)\n",
        "    x = layers.Activation('relu')(layers.BatchNormalization(axis=-1)(x))\n",
        "    x = convolution(x, numberFilters, 1, weigthDecay, p)\n",
        "    return layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "def Dense_Net_INIT(input, numberOfLabels, depth=40, blockCount=3, growthRate=12, filterCount=16, bottleneck=False, compression=1.0, p=None, weigthDecay=0):\n",
        "    numberOfLayers = [int((depth - 4) / blockCount)] * blockCount\n",
        "\n",
        "    x = convolution(input, filterCount, 3, weigthDecay, 0)\n",
        "    for i,block in enumerate(numberOfLayers):\n",
        "        x = denselyConnectedBlock(x, block, growthRate, bottleneck=bottleneck, p=p, weigthDecay=weigthDecay)\n",
        "        if i != len(numberOfLayers)-1:\n",
        "            x = TransBlock(x, compression=compression, p=p, weigthDecay=weigthDecay)\n",
        "\n",
        "    x = layers.Activation('relu')(layers.BatchNormalization(axis=-1)(x))\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return layers.Dense(numberOfLabels, activation='softmax', kernel_regularizer=regularizers.l2(weigthDecay))(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko99DuNLoskt"
      },
      "source": [
        "input = layers.Input(shape=(32, 32, 3))\n",
        "output = Dense_Net_INIT(input, 10, depth = 100, growthRate=36)\n",
        "model = models.Model(input, output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdcTVvTosku",
        "outputId": "85ba46a3-abfa-45cd-ec19-7e13d7bf4ecb"
      },
      "source": [
        "if RUN:\n",
        "    denseNet = TrainableModel(dataset, model, name = \"DenseNet\")\n",
        "    denseNet.optimizer = tf.keras.optimizers.SGD(0.1, 0.9, nesterov=True)\n",
        "    denseNet.compileModel()\n",
        "    denseNet.fit(num_epochs = 100)\n",
        "    denseNet.evaluate()\n",
        "    denseNet.submission()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2500/2500 [==============================] - ETA: 0s - loss: 1.8133 - accuracy: 0.3853WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_test_batch_end` time: 0.1727s). Check your callbacks.\n",
            "2500/2500 [==============================] - 1672s 669ms/step - loss: 1.8133 - accuracy: 0.3853 - val_loss: 1.4390 - val_accuracy: 0.5139\n",
            "Epoch 2/100\n",
            "2500/2500 [==============================] - 1672s 669ms/step - loss: 1.1764 - accuracy: 0.5813 - val_loss: 1.3135 - val_accuracy: 0.6070\n",
            "Epoch 3/100\n",
            "2500/2500 [==============================] - 1673s 669ms/step - loss: 0.8934 - accuracy: 0.6867 - val_loss: 0.9365 - val_accuracy: 0.6856\n",
            "Epoch 4/100\n",
            "2500/2500 [==============================] - 1679s 671ms/step - loss: 0.7144 - accuracy: 0.7519 - val_loss: 0.7266 - val_accuracy: 0.7506\n",
            "Epoch 5/100\n",
            "2500/2500 [==============================] - 1679s 671ms/step - loss: 0.5905 - accuracy: 0.7926 - val_loss: 0.5825 - val_accuracy: 0.7990\n",
            "Epoch 6/100\n",
            "2500/2500 [==============================] - 1679s 672ms/step - loss: 0.4967 - accuracy: 0.8267 - val_loss: 0.8958 - val_accuracy: 0.7241\n",
            "Epoch 7/100\n",
            "2500/2500 [==============================] - 1679s 671ms/step - loss: 0.4957 - accuracy: 0.8274 - val_loss: 1.3822 - val_accuracy: 0.7549\n",
            "Epoch 8/100\n",
            "2500/2500 [==============================] - 1680s 672ms/step - loss: 0.3960 - accuracy: 0.8635 - val_loss: 0.6209 - val_accuracy: 0.7992\n",
            "Epoch 9/100\n",
            "2500/2500 [==============================] - 1680s 672ms/step - loss: 0.3193 - accuracy: 0.8882 - val_loss: 1.9272 - val_accuracy: 0.7675\n",
            "Epoch 10/100\n",
            "2500/2500 [==============================] - 1682s 673ms/step - loss: 0.2572 - accuracy: 0.9081 - val_loss: 0.5740 - val_accuracy: 0.8206\n",
            "Epoch 11/100\n",
            "2500/2500 [==============================] - 1679s 671ms/step - loss: 0.2088 - accuracy: 0.9256 - val_loss: 0.6205 - val_accuracy: 0.8414\n",
            "Epoch 12/100\n",
            "2500/2500 [==============================] - 1681s 672ms/step - loss: 0.1953 - accuracy: 0.9312 - val_loss: 0.5294 - val_accuracy: 0.8423\n",
            "Epoch 13/100\n",
            "2500/2500 [==============================] - 1679s 672ms/step - loss: 0.1467 - accuracy: 0.9493 - val_loss: 0.5619 - val_accuracy: 0.8406\n",
            "Epoch 14/100\n",
            "2500/2500 [==============================] - 1678s 671ms/step - loss: 0.1125 - accuracy: 0.9608 - val_loss: 0.6559 - val_accuracy: 0.8307\n",
            "Epoch 15/100\n",
            "2500/2500 [==============================] - 1678s 671ms/step - loss: 0.1064 - accuracy: 0.9626 - val_loss: 0.7293 - val_accuracy: 0.8358\n",
            "Epoch 16/100\n",
            "2500/2500 [==============================] - 1677s 671ms/step - loss: 0.0715 - accuracy: 0.9742 - val_loss: 0.5755 - val_accuracy: 0.8486\n",
            "Epoch 17/100\n",
            "2500/2500 [==============================] - 1677s 671ms/step - loss: 0.0533 - accuracy: 0.9825 - val_loss: 0.6209 - val_accuracy: 0.8511\n",
            "Epoch 18/100\n",
            "2500/2500 [==============================] - 1675s 670ms/step - loss: 0.0474 - accuracy: 0.9838 - val_loss: 0.8905 - val_accuracy: 0.8409\n",
            "Epoch 19/100\n",
            "2500/2500 [==============================] - 1675s 670ms/step - loss: 0.0460 - accuracy: 0.9844 - val_loss: 0.6282 - val_accuracy: 0.8602\n",
            "Epoch 20/100\n",
            "2500/2500 [==============================] - 1677s 671ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.6423 - val_accuracy: 0.8604\n",
            "Epoch 21/100\n",
            "2500/2500 [==============================] - 1676s 670ms/step - loss: 0.0288 - accuracy: 0.9906 - val_loss: 0.8838 - val_accuracy: 0.8400\n",
            "Epoch 22/100\n",
            "1297/2500 [==============>...............] - ETA: 12:30 - loss: 0.0308 - accuracy: 0.9897"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNPwQB0iqtK4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}